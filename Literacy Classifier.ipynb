{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "df = pd.read_csv('Literacy Classifier Final Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Social Group', 'Rural/Urban', 'State', 'Gender', 'Age',\n",
      "       'Internet Access', 'Computer Access', 'Marital Status', 'Literacy'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \n",
    "\n",
    "df['Digital Access'] = df['Internet Access'] + df['Computer Access']\n",
    "\n",
    "# Create Age Brackets\n",
    "bins = [5, 18, 35, 60, 118]\n",
    "labels = ['<18', '18-35', '35-60', '>60']\n",
    "df['Age Bracket'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "state_to_region = {\n",
    "    1: 'North India',      # Jammu & Kashmir\n",
    "    2: 'North India',      # Himachal Pradesh\n",
    "    3: 'North India',      # Punjab\n",
    "    4: 'North India',      # Chandigarh\n",
    "    5: 'North India',      # Uttarakhand (Uttaranchal)\n",
    "    6: 'North India',      # Haryana\n",
    "    7: 'North India',      # Delhi\n",
    "    8: 'North India',      # Rajasthan\n",
    "    9: 'North India',      # Uttar Pradesh\n",
    "    10: 'East India',      # Bihar\n",
    "    11: 'Northeast India', # Sikkim\n",
    "    12: 'Northeast India', # Arunachal Pradesh\n",
    "    13: 'Northeast India', # Nagaland\n",
    "    14: 'Northeast India', # Manipur\n",
    "    15: 'Northeast India', # Mizoram\n",
    "    16: 'Northeast India', # Tripura\n",
    "    17: 'Northeast India', # Meghalaya\n",
    "    18: 'Northeast India', # Assam\n",
    "    19: 'East India',      # West Bengal\n",
    "    20: 'East India',      # Jharkhand\n",
    "    21: 'East India',      # Odisha\n",
    "    22: 'Central India',   # Chhattisgarh\n",
    "    23: 'Central India',   # Madhya Pradesh\n",
    "    24: 'West India',      # Gujarat\n",
    "    25: 'Union Territories', # Daman & Diu\n",
    "    26: 'Union Territories', # Dadra and Nagar Haveli\n",
    "    27: 'West India',      # Maharashtra\n",
    "    28: 'South India',     # Andhra Pradesh\n",
    "    29: 'South India',     # Karnataka\n",
    "    30: 'West India',      # Goa\n",
    "    31: 'Union Territories', # Lakshadweep\n",
    "    32: 'South India',     # Kerala\n",
    "    33: 'South India',     # Tamil Nadu\n",
    "    34: 'South India',     # Pondicherry\n",
    "    35: 'Union Territories', # Andaman and Nicobar Islands\n",
    "    36: 'South India'      # Telangana\n",
    "}\n",
    "df['Region'] = df['State'].map(state_to_region)\n",
    "df[['State', 'Region']].drop_duplicates()\n",
    "\n",
    "# Drop redundant features after feature engineering\n",
    "df.drop(['Internet Access', 'Computer Access', 'Age', 'State'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Social Group  Rural/Urban  Gender  Marital Status  Literacy  \\\n",
      "0             9            1       1               2         1   \n",
      "1             9            1       2               2         1   \n",
      "2             9            1       1               2         1   \n",
      "3             9            1       2               2         1   \n",
      "4             9            1       2               1         1   \n",
      "\n",
      "   Digital Access Age Bracket       Region  \n",
      "0               0         >60  North India  \n",
      "1               0         >60  North India  \n",
      "2               0       35-60  North India  \n",
      "3               0       35-60  North India  \n",
      "4               0         <18  North India  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())     # Verifying the first few rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 480938 entries, 0 to 480937\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count   Dtype   \n",
      "---  ------          --------------   -----   \n",
      " 0   Social Group    480938 non-null  int64   \n",
      " 1   Rural/Urban     480938 non-null  int64   \n",
      " 2   Gender          480938 non-null  int64   \n",
      " 3   Marital Status  480938 non-null  int64   \n",
      " 4   Literacy        480938 non-null  int64   \n",
      " 5   Digital Access  480938 non-null  int64   \n",
      " 6   Age Bracket     480938 non-null  category\n",
      " 7   Region          480938 non-null  object  \n",
      "dtypes: category(1), int64(6), object(1)\n",
      "memory usage: 26.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info()) # Looking at info of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Social Group', 'Rural/Urban', 'Gender', 'Marital Status', 'Digital Access', 'Age Bracket', 'Region']]\n",
    "y = df['Literacy']                \n",
    "# Splitting the data into the feature and targe sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first split off the training set (60% of the data)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# Then we split the remaining data into the cross validation and test set\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_temp, y_temp, test_size=0.25, random_state=1)\n",
    "\n",
    "del X_temp, y_temp\n",
    "\n",
    "# So, X_cv is 20% of the original data, X_train is 60%, X_test is 20%, and vice versa for y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [['Social Group', 'Rural/Urban', 'Gender', 'Marital Status', 'Digital Access', 'Age Bracket', 'Region']]\n",
    "numerical_columns = []\n",
    "\n",
    "# Our only columns are categorical, so we don't need to scale any numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# List all categorical columns\n",
    "categorical_columns = X_train.columns.tolist()  # Automatically fetch all column names\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(), categorical_columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Columns:\n",
      "['onehot__Social Group_1' 'onehot__Social Group_2'\n",
      " 'onehot__Social Group_3' 'onehot__Social Group_9' 'onehot__Rural/Urban_1'\n",
      " 'onehot__Rural/Urban_2' 'onehot__Gender_1' 'onehot__Gender_2'\n",
      " 'onehot__Marital Status_1' 'onehot__Marital Status_2'\n",
      " 'onehot__Marital Status_3' 'onehot__Digital Access_0'\n",
      " 'onehot__Digital Access_1' 'onehot__Digital Access_2'\n",
      " 'onehot__Age Bracket_18-35' 'onehot__Age Bracket_35-60'\n",
      " 'onehot__Age Bracket_<18' 'onehot__Age Bracket_>60'\n",
      " 'onehot__Region_Central India' 'onehot__Region_East India'\n",
      " 'onehot__Region_North India' 'onehot__Region_Northeast India'\n",
      " 'onehot__Region_South India' 'onehot__Region_Union Territories'\n",
      " 'onehot__Region_West India']\n"
     ]
    }
   ],
   "source": [
    "# First we fit on the training data\n",
    "X_train_enc = ct.fit_transform(X_train)\n",
    " \n",
    "#Then we also transform the cross validation and test sets\n",
    "X_cv_enc = ct.transform(X_cv)\n",
    "X_test_enc = ct.transform(X_test)\n",
    "\n",
    "encoded_column_names = ct.get_feature_names_out()\n",
    "\n",
    "# Print the column names\n",
    "print(\"Encoded Columns:\")\n",
    "print(encoded_column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_enc, y_train)\n",
    "# Create synthentic samples for the minority class (Illiterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 384us/step - accuracy: 0.6603 - loss: 0.9056 - val_accuracy: 0.4889 - val_loss: 0.9558\n",
      "Epoch 2/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 374us/step - accuracy: 0.6883 - loss: 0.5480 - val_accuracy: 0.4983 - val_loss: 0.9321\n",
      "Epoch 3/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382us/step - accuracy: 0.6886 - loss: 0.5401 - val_accuracy: 0.5043 - val_loss: 0.9395\n",
      "Epoch 4/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 381us/step - accuracy: 0.6887 - loss: 0.5356 - val_accuracy: 0.4920 - val_loss: 0.9420\n",
      "Epoch 5/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 371us/step - accuracy: 0.6879 - loss: 0.5327 - val_accuracy: 0.4836 - val_loss: 0.9752\n",
      "Epoch 6/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 370us/step - accuracy: 0.6896 - loss: 0.5310 - val_accuracy: 0.5028 - val_loss: 0.9309\n",
      "Epoch 7/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 371us/step - accuracy: 0.6896 - loss: 0.5291 - val_accuracy: 0.4830 - val_loss: 0.9822\n",
      "Epoch 8/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 372us/step - accuracy: 0.6907 - loss: 0.5287 - val_accuracy: 0.4757 - val_loss: 1.0297\n",
      "Epoch 9/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 373us/step - accuracy: 0.6911 - loss: 0.5279 - val_accuracy: 0.4861 - val_loss: 0.9593\n",
      "Epoch 10/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 369us/step - accuracy: 0.6894 - loss: 0.5285 - val_accuracy: 0.4843 - val_loss: 0.9868\n",
      "Epoch 11/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 373us/step - accuracy: 0.6917 - loss: 0.5254 - val_accuracy: 0.5031 - val_loss: 0.9174\n",
      "Epoch 12/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 377us/step - accuracy: 0.6908 - loss: 0.5269 - val_accuracy: 0.5300 - val_loss: 0.9102\n",
      "Epoch 13/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 375us/step - accuracy: 0.6912 - loss: 0.5254 - val_accuracy: 0.4993 - val_loss: 0.9799\n",
      "Epoch 14/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 396us/step - accuracy: 0.6904 - loss: 0.5267 - val_accuracy: 0.4953 - val_loss: 0.9199\n",
      "Epoch 15/15\n",
      "\u001b[1m14912/14912\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 385us/step - accuracy: 0.6914 - loss: 0.5252 - val_accuracy: 0.5272 - val_loss: 0.8870\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "# Convert to numpy array for compatibility with class weight calculation\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "# Incentivize not misclassifying the minority class\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_enc.shape[1],), kernel_regularizer=l2(0.01)),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),               \n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']            \n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5,  # Stop if val_loss doesn't improve for 5 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_resampled, y_resampled, \n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    class_weight = class_weights_dict,\n",
    "    validation_data=(X_cv_enc, y_cv),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1503/1503\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step\n",
      "Confusion Matrix:\n",
      " [[ 7538   791]\n",
      " [15411 24354]]\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.91      0.48      8329\n",
      "           1       0.97      0.61      0.75     39765\n",
      "\n",
      "    accuracy                           0.66     48094\n",
      "   macro avg       0.65      0.76      0.62     48094\n",
      "weighted avg       0.86      0.66      0.70     48094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_probs = model.predict(X_test_enc)\n",
    "y_test_pred = (y_test_probs > 0.30).astype(int) # Threshold of 0.30 maximizes F1 score\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "print(\"Report:\\n\", classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed input: [[1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      "  0.]]\n",
      "Processed input shape: (1, 25)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Raw prediction: [[0.00549285]]\n",
      "Probability: 0.00549284927546978\n",
      "Binary Prediction (Class): 0\n",
      "Literacy Status: Illiterate\n"
     ]
    }
   ],
   "source": [
    "# Testing out the model while also engineeringg the input\n",
    "\n",
    "model_columns = [\n",
    "    'onehot__Social Group_1', 'onehot__Social Group_2',\n",
    "    'onehot__Social Group_3', 'onehot__Social Group_9', 'onehot__Rural/Urban_1',\n",
    "    'onehot__Rural/Urban_2', 'onehot__Gender_1', 'onehot__Gender_2',\n",
    "    'onehot__Marital Status_1', 'onehot__Marital Status_2',\n",
    "    'onehot__Marital Status_3', 'onehot__Digital Access_0',\n",
    "    'onehot__Digital Access_1', 'onehot__Digital Access_2',\n",
    "    'onehot__Age Bracket_18-35', 'onehot__Age Bracket_35-60',\n",
    "    'onehot__Age Bracket_<18', 'onehot__Age Bracket_>60',\n",
    "    'onehot__Region_Central India', 'onehot__Region_East India',\n",
    "    'onehot__Region_North India', 'onehot__Region_Northeast India',\n",
    "    'onehot__Region_South India', 'onehot__Region_Union Territories',\n",
    "    'onehot__Region_West India'\n",
    "]\n",
    "\n",
    "sample_input = {\n",
    "  \"social_group\": \"Scheduled Tribes\",\n",
    "  \"rural_urban\": \"Rural\",\n",
    "  \"state\": \"Uttar Pradesh\",\n",
    "  \"gender\": \"Female\",\n",
    "  \"age\": 60,\n",
    "  \"internet_access\": \"No\",\n",
    "  \"computer_access\": \"No\",\n",
    "  \"marital_status\": \"Widowed\"\n",
    "}\n",
    "\n",
    "def preprocess_input(data: dict):\n",
    "\n",
    "    # Mapping for state to region\n",
    "    state_to_region = {\n",
    "        \"Jammu & Kashmir\": \"North India\",\n",
    "        \"Himachal Pradesh\": \"North India\",\n",
    "        \"Punjab\": \"North India\",\n",
    "        \"Chandigarh\": \"North India\",\n",
    "        \"Uttarakhand\": \"North India\",\n",
    "        \"Haryana\": \"North India\",\n",
    "        \"Delhi\": \"North India\",\n",
    "        \"Rajasthan\": \"North India\",\n",
    "        \"Uttar Pradesh\": \"North India\",\n",
    "        \"Bihar\": \"East India\",\n",
    "        \"Sikkim\": \"Northeast India\",\n",
    "        \"Arunachal Pradesh\": \"Northeast India\",\n",
    "        \"Nagaland\": \"Northeast India\",\n",
    "        \"Manipur\": \"Northeast India\",\n",
    "        \"Mizoram\": \"Northeast India\",\n",
    "        \"Tripura\": \"Northeast India\",\n",
    "        \"Meghalaya\": \"Northeast India\",\n",
    "        \"Assam\": \"Northeast India\",\n",
    "        \"West Bengal\": \"East India\",\n",
    "        \"Jharkhand\": \"East India\",\n",
    "        \"Odisha\": \"East India\",\n",
    "        \"Chhattisgarh\": \"Central India\",\n",
    "        \"Madhya Pradesh\": \"Central India\",\n",
    "        \"Gujarat\": \"West India\",\n",
    "        \"Daman & Diu\": \"Union Territories\",\n",
    "        \"Dadara and Nagar Haveli\": \"Union Territories\",\n",
    "        \"Maharashtra\": \"West India\",\n",
    "        \"Andhra Pradesh\": \"South India\",\n",
    "        \"Karnataka\": \"South India\",\n",
    "        \"Goa\": \"West India\",\n",
    "        \"Lakshadweep\": \"Union Territories\",\n",
    "        \"Kerala\": \"South India\",\n",
    "        \"Tamil Nadu\": \"South India\",\n",
    "        \"Pondicherry\": \"South India\",\n",
    "        \"Andaman and Nicobar Islands\": \"Union Territories\",\n",
    "        \"Telangana\": \"South India\",\n",
    "    }\n",
    "\n",
    "    social_group_mapping = {\n",
    "        \"Scheduled Tribes\": 1,\n",
    "        \"Scheduled Castes\": 2,\n",
    "        \"Other Backward Classes\": 3,\n",
    "        \"Others\": 9\n",
    "    }\n",
    "    marital_status_mapping = {\n",
    "        \"Single\": 1,\n",
    "        \"Married\": 2,\n",
    "        \"Widowed\": 3\n",
    "    }\n",
    "    \n",
    "    rural_urban_mapping = {\n",
    "        \"Rural\": 1,\n",
    "        \"Urban\": 2\n",
    "    }\n",
    "\n",
    "    gender_mapping = {\n",
    "        \"Male\": 1,\n",
    "        \"Female\": 2\n",
    "    }\n",
    "    \n",
    "    def bin_age(age):\n",
    "        if age < 18:\n",
    "            return '<18'\n",
    "        elif 18 <= age < 35:\n",
    "            return '18-35'\n",
    "        elif 35 <= age < 60:\n",
    "            return '35-60'\n",
    "        else:\n",
    "            return '>60'\n",
    "\n",
    "    digital_access = 0\n",
    "    if data.get(\"internet_access\") == \"Yes\":\n",
    "        digital_access += 1\n",
    "    if data.get(\"computer_access\") == \"Yes\":\n",
    "        digital_access += 1\n",
    "\n",
    "    region = state_to_region.get(data.get(\"state\", \"\"), \"Unknown\")\n",
    "    age_bracket = bin_age(data.get(\"age\", 0))\n",
    "    social_group = social_group_mapping.get(data.get(\"social_group\", \"\"))\n",
    "    marital_status = marital_status_mapping.get(data.get(\"marital_status\", \"\"))\n",
    "    rural_urban = rural_urban_mapping.get(data.get(\"rural_urban\", \"\"), \"Unknown\")\n",
    "    gender = gender_mapping.get(data.get(\"gender\", \"\"), \"Unknown\")\n",
    "\n",
    "    categorical_features = {\n",
    "        \"social_group\": social_group,\n",
    "        \"rural_urban\": rural_urban,\n",
    "        \"gender\": gender,\n",
    "        \"marital_status\": marital_status,\n",
    "        \"digital_access\": digital_access, \n",
    "        \"age_bracket\": age_bracket,\n",
    "        \"region\": region\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame for one-hot encoding\n",
    "    df = pd.DataFrame([categorical_features])\n",
    "\n",
    "    # Apply one-hot encoding with consistent prefixes\n",
    "    df_encoded = pd.get_dummies(\n",
    "        df,\n",
    "        columns=[\"social_group\", \"rural_urban\", \"gender\", \"marital_status\", \"digital_access\", \"age_bracket\", \"region\"],\n",
    "        prefix=[\"onehot__Social Group\", \"onehot__Rural/Urban\", \"onehot__Gender\", \"onehot__Marital Status\", \"onehot__Digital Access\", \"onehot__Age Bracket\", \"onehot__Region\"]\n",
    "    )\n",
    "\n",
    "    # Align columns with the model's expected input columns\n",
    "    df_encoded = df_encoded.reindex(columns=model_columns, fill_value=0)\n",
    "    \n",
    "    return df_encoded.values.astype(np.float32)\n",
    "    \n",
    "\n",
    "# Step 1: Preprocess the input\n",
    "try:\n",
    "    processed_sample_input = preprocess_input(sample_input)\n",
    "    print(f\"Processed input: {processed_sample_input}\")\n",
    "    print(f\"Processed input shape: {processed_sample_input.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during preprocessing: {e}\")\n",
    "\n",
    "# Step 2: Test the model prediction\n",
    "try:\n",
    "    # Ensure the model is loaded\n",
    "    if 'model' not in globals():\n",
    "        model = tf.keras.models.load_model('literacy_classifier.keras')\n",
    "\n",
    "    # Get the raw probability\n",
    "    prediction = model.predict(processed_sample_input)\n",
    "    print(f\"Raw prediction: {prediction}\")\n",
    "\n",
    "    # Extract the probability if necessary\n",
    "    raw_prediction = float(prediction[0]) if prediction.ndim == 1 else float(prediction[0][0])\n",
    "\n",
    "    # Step 3: Apply the threshold\n",
    "    threshold = 0.30  # Using the same threshold as we did in the evlaution\n",
    "    binary_prediction = 1 if raw_prediction > threshold else 0\n",
    "    literacy_status = \"Literate\" if binary_prediction == 1 else \"Illiterate\"\n",
    "\n",
    "    # Step 4: Display the results\n",
    "    print(f\"Probability: {raw_prediction}\")\n",
    "    print(f\"Binary Prediction (Class): {binary_prediction}\")\n",
    "    print(f\"Literacy Status: {literacy_status}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('literacy_classifier.keras')\n",
    "\n",
    "# Saved the model to a .keras file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
